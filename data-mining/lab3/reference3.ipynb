{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация\n",
    "\n",
    "Кластеризация — это задача группировки множества объектов на подмножества (кластеры) таким образом, чтобы объекты из одного кластера были более похожи друг на друга, чем на объекты из других кластеров по какому-либо критерию.\n",
    "\n",
    "Задача кластеризации относится к классу задач машинного обучения без учителя (unsupervised learning).\n",
    "\n",
    "Часто кластеризация выступает первым шагом при анализе данных. После выделения схожих групп к ним применяются другие методы машинного обучения, для каждой группы строится отдельная модель. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство алгоритмов кластеризации предполагают сравнение объектов между собой на основе некоторой меры близости (сходства). Мера близости убывает (иногда возрастает) с увеличением близости объектов.\n",
    "\n",
    "Импортируем необходимые для работы библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация наборов данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бибиотека scikit-learn включает в себя различные генераторы случайных данных, которые можно использовать для создания синтетических наборов данных контролируемого размера и сложности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `make_blobs()` создает наборы данных с гаусовым распределением относительно заданных центров (опускаем метки классов для созданных точек):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "centers3 = [[2, 2], [-2, -2], [2, -2]]\n",
    "X, _ = make_blobs(n_samples=300, centers=centers3, cluster_std=0.7, random_state=0)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для визуализации построенного набора данных используем следующий программный код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:,0],X[:,1],edgecolors='r',c='y',s=500)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `make_classification()` создает случайную задачу классификации, а именно, вокруг вершин многомерного гиперкуба создаются нормально распределенные группы точек, при этом вводится зависимость между случайными величинами и к данным добавляется дополнительный шум:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, \n",
    "                           n_clusters_per_class=1, random_state=4)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(X[:,0],X[:,1],edgecolors='k',c='g',s=100)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи функции `make_circles()` могут быть созданы случайные наборы данных в виде концентрических окружностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, _ = make_circles(n_samples=1000, factor=.5, noise=.05)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0],X[:,1],edgecolors='k',c='y',s=100)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи функции `make_moons()` могут быть созданы случайные наборы данных в виде двух разделенных полуокружностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, _ = make_moons(n_samples=500, noise=.05)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:,0],X[:,1],edgecolors='k',c='violet',s=200)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестовый набор данных и его визуализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать для визуализации следующую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster(X,y):\n",
    "    # визуализация кластеров \n",
    "    plt.figure(figsize=(12,10))\n",
    "    clusters = np.unique(y)\n",
    "    for cluster in clusters:\n",
    "        # индексы строк для данного кластера \n",
    "        row_ix = np.where(y == cluster)\n",
    "        # диаграмма рассеяния для кластера\n",
    "        plt.scatter(X[row_ix, 0], X[row_ix, 1],s=100)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем выполнять кластеризацию для следующего набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# синтетический набор данных\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, \n",
    "                           n_clusters_per_class=1, random_state=4)\n",
    "\n",
    "plot_cluster(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм K-средних (K-means) является наиболее известным алгоритмом кластеризации и базируется на идее  минимизации дисперсии в каждом кластере (минимизации суммарного квадратичного отклонения точек кластеров от центров этих кластеров).\n",
    "\n",
    "Основным параметром конфигурации является параметр `n_clusters`, равный оценочному количеству кластеров в данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кластеризация k-means \n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель\n",
    "model = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тренируем модель\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# присваиваем метку каждой точке набора\n",
    "yhat = model.predict(X)\n",
    "\n",
    "plot_cluster(X,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для рассматриваемого набора данных получено разбиение на два кластера, однако разная дисперсия первоначальных классов приводит к не самому лучшему результату."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У класса KMeans имеются следующие свойства:\n",
    "\n",
    "* `cluster_centers_` – массив центров кластеров  \n",
    "* `labels_` – метки для каждой точки\n",
    "* `inertia_` – сумма квадратов расстояний от точек до ближайших центров кластеров\n",
    "* `n_iter_` – количество итераций алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно добавить на визуализацию кластеризации набора данных центры кластеров: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(X,yhat)\n",
    "centers = model.cluster_centers_\n",
    "plt.scatter(\n",
    "    centers[:, 0],\n",
    "    centers[:, 1],\n",
    "    marker=\"D\",\n",
    "    s=500,\n",
    "    color=\"k\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно выполнить визуализацию областей принятия решений о кластеризации точек данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "#plt.figure(1)\n",
    "#plt.clf()\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    interpolation=\"nearest\",\n",
    "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "    cmap=plt.cm.Paired,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], \"yo\", markersize=5)\n",
    "# Plot the centroids as a white X\n",
    "centers = model.cluster_centers_\n",
    "plt.scatter(\n",
    "    centers[:, 0],\n",
    "    centers[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=1000,\n",
    "    lw=5,\n",
    "    color=\"r\",\n",
    ")\n",
    "plt.title(\"Визуализация областей принятия решений при кластеризации методом K-means\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм mini-batch K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм mini-batch K-Means - это модифицированная версия алгоритма k-средних, которая обновляет центроиды кластеров, используя мини выборки из набора вместо всего набора данных, что может ускорить работу с большими наборами данных и, возможно, обеспечить более высокую устойчивость к статистическому шуму  в данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кластеризация mini-batch k-means\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# модель\n",
    "model = MiniBatchKMeans(n_clusters=2)\n",
    "\n",
    "# тренируем модель\n",
    "model.fit(X)\n",
    "\n",
    "# присваиваем метку каждой точке набора\n",
    "yhat = model.predict(X)\n",
    "\n",
    "plot_cluster(X,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат кластеризации аналогичен результату применения алгоритма K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм Mean Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В алгоритме Mean Shift (сдвиг среднего значения) центроиды кластеров выбираются в областях с наибольшей плотностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кластеризация mean shift \n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "# модель\n",
    "model = MeanShift()\n",
    "\n",
    "# тренируем модель и присваиваем метку каждой точке набора\n",
    "yhat = model.fit_predict(X)\n",
    "\n",
    "plot_cluster(X,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В классе `MeanShift` имеются следующие свойства:\n",
    "\n",
    "* `cluster_centers_` – массив центров кластеров  \n",
    "* `labels_` – метки для каждой точки\n",
    "* `n_iter_` – количество итераций алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Иерархическая кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При иерархической кластеризации создается иерархия вложенных кластеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# иерархическая кластеризация\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# модель иерархической кластеризации, расстояние между кластерами методом Ward\n",
    "model = AgglomerativeClustering(n_clusters=2)\n",
    "\n",
    "# тренируем модель и присваиваем метку каждой точке набора\n",
    "yhat = model.fit_predict(X)\n",
    "\n",
    "plot_cluster(X,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В классе AgglomerativeClustering имеются, в частности, следующие свойства:\n",
    "\n",
    "* `n_cluster_` – количество кластеров, найденных алгоритмом  \n",
    "* `labels_` – метки для каждой точки\n",
    "\n",
    "Для иерархической кластеризации имеется полезный способ визуализации в виде дендрограммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "model = model.fit(X)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Дендрограмма иерархической кластеризации\")\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode=\"level\", p=3)\n",
    "plt.xlabel(\"Число точек в узле (или индекс точки, если без скобок)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм BIRCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм BIRCH (balanced iterative reducing and clustering using hierarchies) — это алгоритм иерархической кластеризации для наборов данных большого размера. Преимуществом BIRCH является возможность динамической  кластеризации по мере поступления новых многомерных данных. В большинстве случаев алгоритм BIRCH требует одного прохода по базе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кластеризация birch \n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "# модель\n",
    "model = Birch(threshold=0.01, n_clusters=2)\n",
    "\n",
    "# тренируем модель\n",
    "model.fit(X)\n",
    "\n",
    "# присваиваем метку каждой точке набора\n",
    "yhat = model.predict(X)\n",
    "\n",
    "plot_cluster(X,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате применения алгоритма получено качественное разбиение на кластеры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В классе Birch имеется, в частности, следующее свойство:\n",
    "\n",
    "* `labels_` – метки для каждой точки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В алгоритме DBSCAN (Density-based spatial clustering of applications with noise) применяется пространственная кластеризация для наборов данных с шумами, основанная на плотности. Алгоритм группирует в один кластер точки в области с высокой плотностью точек. Одиноко расположенные точки помечаются как шум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кластеризация dbscan \n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# модель\n",
    "model = DBSCAN(eps=0.20, min_samples=5)\n",
    "\n",
    "# тренируем модель и присваиваем метку каждой точке набора\n",
    "yhat = model.fit_predict(X)\n",
    "\n",
    "plot_cluster(X,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `eps` представляет собой максимальное расстояние между точками, для которого одна точка будет считаться находящейся в окрестности другой. Параметр `min_samples` означает минимальное количество точек в окрестности заданной точки для того, чтобы она рассматривалась как опорная для алгоритма.\n",
    "\n",
    "В классе DBSCAN имеется, в частности, следующее свойство:\n",
    "\n",
    "* `labels_` – метки для каждой точки\n",
    "\n",
    "Чтобы вывести метки кластеров, количество кластеров, а также долю точек данных, которые кластеризовать не удалось, можно поступить так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Метки кластеров:\", set(model.labels_))\n",
    "print(\"Кол-во кластеров:\", len(set(model.labels_)) - 1) \n",
    "print(\"Доля некластеризованных точек:\", \n",
    "      list(model.labels_).count(-1) / len(list(model.labels_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм OPTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм OPTICS тесно связан с алгоритмом DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кластеризация optics \n",
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "# модель\n",
    "model = OPTICS(eps=0.20, min_samples=5)\n",
    "\n",
    "# тренируем модель и присваиваем метку каждой точке набора\n",
    "yhat = model.fit_predict(X)\n",
    "\n",
    "plot_cluster(X,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры `eps` и `min_samples` те же, что и в алгоритме DBSCAN. \n",
    "\n",
    "В классе OPTICS имеется, в частности, следующее свойство:\n",
    "\n",
    "* `labels_` – метки для каждой точки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Метки кластеров:\", set(model.labels_))\n",
    "print(\"Кол-во кластеров:\", len(set(model.labels_)) - 1) \n",
    "print(\"Доля некластеризованных точек:\", list(model.labels_).count(-1) / len(list(model.labels_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм Affinity Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В алгоритме Affinity Propagation (распространение похожести) между парами объектов распространяются сообщения о похожести  для выбора типичных представителей каждого кластера. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кластеризация affinity propagation \n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# модель\n",
    "model = AffinityPropagation(damping=0.5,random_state=0)\n",
    "\n",
    "# тренируем модель\n",
    "model.fit(X)\n",
    "\n",
    "# присваиваем метку каждой точке набора\n",
    "yhat = model.predict(X)\n",
    "\n",
    "plot_cluster(X,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `damping` представляет собой коэффициент демпфирования между 0.5 и 1, необходимый для избежания колебаний при применении метода.\n",
    "\n",
    "В классе AffinityPropagation имеется, в частности, следующее свойство:\n",
    "\n",
    "* `labels_` – метки для каждой точки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Метки кластеров:\", set(model.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В алгоритме Spectral Clustering (спектральная кластеризация) используются собственные значения матрицы расстояний для понижения размерности перед использованием других методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# спектральная кластеризация \n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# модель\n",
    "model = SpectralClustering(n_clusters=2)\n",
    "\n",
    "# тренируем модель и присваиваем метку каждой точке набора\n",
    "yhat = model.fit_predict(X)\n",
    "\n",
    "plot_cluster(X,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В классе SpectralClustering имеется, в частности, следующее свойство:\n",
    "\n",
    "* `labels_` – метки для каждой точки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Метки кластеров:\", set(model.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель гауссовской смеси (Gaussian Mixture Model) использует допущение, что каждый кластер характеризуется многомерным нормальным распределением со своим матожиданием и ковариационной матрицей, а плотность распределения набора данных представляет собой взвешенную сумму плотностей распределения для всех кластеров. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кластеризация при помощи гауссовских смесей\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# модель\n",
    "model = GaussianMixture(n_components=2)\n",
    "\n",
    "# тренируем модель\n",
    "model.fit(X)\n",
    "\n",
    "# присваиваем метку каждой точке набора\n",
    "yhat = model.predict(X)\n",
    "\n",
    "plot_cluster(X,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для рассматриваемого набора данных кластеры идентифицированы идеально. Это связано с тем, что набор данных был создан как смесь нормальных распределений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цвета визуализации кластеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Команды `matplotlib`, которые принимают в качестве аргумента цвета, могут использовать несколько форматов для указания цветов. Для основных встроенных цветов можно использовать одну букву:\n",
    "\n",
    "b: blue  \n",
    "g: green  \n",
    "r: red  \n",
    "c: cyan  \n",
    "m: magenta  \n",
    "y: yellow  \n",
    "k: black  \n",
    "w: white  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оттенки серого могут быть представлены в виде строки, представляющей число с плавающей запятой в диапазоне от 0 до 1, например:\n",
    "\n",
    "`color = '0.75'`\n",
    "\n",
    "Вы можете указать цвет, используя шестнадцатеричное представление цвета в html, например:\n",
    "\n",
    "`color = '#eeefff'`\n",
    "\n",
    "или можно использовать кортеж вида `(R,G,B)`, где каждое из чисел R,G,B находится в диапазоне от 0 до 1.\n",
    "\n",
    "Наконец, поддерживаются допустимые полные названия цветов из html, такие как  ‘red’, ‘burlywood’ and ‘chartreuse’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим следующий пример визуализации кластеризации с использованием различных цветов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем синтетический набор данных\n",
    "centers = [[0, 0], [3, 0], [6, 0], [0, 3], [3, 3], [6, 3], [0, 6], [3, 6], [6, 6]]\n",
    "X, labels_true = make_blobs(n_samples=500, centers=centers, cluster_std=.75,\n",
    "                            random_state=0)\n",
    "\n",
    "# кластеризация Affinity Propagation\n",
    "af = AffinityPropagation(damping=0.97,random_state=0).fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    class_members = labels == k\n",
    "    cluster_center = X[cluster_centers_indices[k]]\n",
    "    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    for x in X[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.title('Количество кластеров: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матрица сопряженности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При оценке качества кластеризации важную роль играет матрица сопряженности (contingency matrix). \n",
    "\n",
    "Для построения матрицы сопряженности можно воспользоваться следующей функцией из библиотеки scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "contingency_matrix(labels_true, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве аргументов функции `contingency_matrix()` подается массив меток классов, известный заранее, и массив меток кластеров, полученный в результате кластеризации. Элементы матрицы сопряженности могут быть использованы для расчета различных мер качества кластеризации.\n",
    "\n",
    "Следует не путать матрицу сопряженности с матрицей ошибок (confusion matrix), которая применяется в задачах классификации и является квадратной (число строк равно числу столбцов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим матрицу сопряженности в следующем виде:\n",
    "\n",
    "<img src=\"Contingency_matrix.pdf\" width=\"400\"/>\n",
    "\n",
    "Здесь $n_{i}=\\left|T_{i}\\right|$ – число точек в классе $T_{i}$ и $m_{j}=\\left|C_{j}\\right|$ – число точек в кластере $C_{j}$, $n$ - общее число точек в наборе данных.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чистота (purity) кластеризации $\\mathcal{C}$ определяется как взвешенная сумма показателей чистоты кластеров:\n",
    "\n",
    "$p=\\frac{1}{n}\\,\\sum_{j=1}^{r}\\max_{i=\\overline{1,k}}n_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для кластера $C_{j}$ обозначим через $i_{j}$ класс, который содержит максимальное количество точек из $C_{j}$, т.е. $i_{j}=\\max_{i=\\overline{1,k}}n_{ij}$. \n",
    "\n",
    "F-мера кластеризации $\\mathcal{C}$ определяется как среднее значение показателей F-мер кластеров:\n",
    "\n",
    "$F=\\frac{1}{r}\\sum_{j=1}^{r}F_{j}, F_{j}=\\frac{2n_{i_{j}j}}{n_{i_{j}}+m_{j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание на лабораторную работу №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для закрепленного за Вами варианта лабораторной работы:\n",
    "\n",
    "1. Считайте из заданного набора данных репозитария UCI значения двух признаков и метки класса. \n",
    "\n",
    "2. Если среди меток класса имеются пропущенные значения, то удалите записи с пропущенными метками класса. Если в признаках имеются пропущенные значения, то замените их на медианные значения того класса, к которому относится запись с пропущенным значением в признаке.\n",
    "\n",
    "3. Если количество различных меток класса больше пяти, то объедините некоторые (наименее многочисленные) классы, чтобы общее количество классов не превышало семь. \n",
    "\n",
    "4. Визуализируйте набор данных в виде точек плоскости с координатами, соответствующими двум признакам, отображая точки различных классов разными цветами. Подпишите оси и рисунок, создайте легенду набора данных. \n",
    "\n",
    "5. Проведите кластеризацию набора данных из двух признаков с помощью алгоритмов, указанных в индивидуальном задании, для случая, когда количество кластеров равно количеству классов в исходном наборе (с учетом корректировки). В случае отсутствия сходимости алгоритма измените аргументы по умолчанию или используйте для кластеризации случайную выборку из набора данных.\n",
    "\n",
    "6. Для каждого из алгоритмов кластеризации, указанных в индивидуальном задании, постройте матрицу сопряженности, используя функцию `contingency_matrix()` из scikit-learn, и найдите значения мер качества кластеризации, указанные в индивидуальном задании.  \n",
    "\n",
    "7. Определите алгоритм кластеризации, оптимальный с точки зрения меры качества кластеризации, указанной в индивидуальном задании.\n",
    "\n",
    "8. Для оптимального алгоритма кластеризации из предыдущего пункта визуализируйте области принятия решения и набор данных в виде точек на плоскости с координатами, соответствующими двум признакам, отображая точки различных кластеров разными цветами. Подпишите оси и рисунок, создайте легенду набора данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
